# Copiloto Conversacional

Proyecto de prueba t√©cnica: asistente conversacional que utiliza **Google Gemini**, **LangChain** y **Chroma DB**.  
Incluye backend en **Flask** y frontend en **React**.

---

## üõ†Ô∏è Tecnolog√≠as utilizadas

- **Backend:** Flask, Flask-CORS  
- **Frontend:** React  
- **Procesamiento de documentos:** PyPDF
- **Framewrok** LangChain
- **Vector DB:** Chroma DB  
- **Modelo generativo:** Google Gemini (Gemini 2.0)  
- **Variables de entorno:** python-dotenv  
- **Contenedores:** Docker y Docker Compose  

---


## üíº Descargar proyecto
En la CMD copie el siguiente comando:
```shell
git clone https://github.com/MoisesGodoy17/copiloto-conversacional.git
cd copiloto-conversacional
```
---

## üìÅ Preparaci√≥n del ambiente 
### Creaci√≥n de las variables de entorno
En la ruta ra√≠z del proyecto `copiloto-conversacional/`, crear una carpeta llamada env-api. Ahora en necesario crear el archivo de variable de entorno. Dentro de la carpeta `env-api/`, crear un archivo llamado `.env`.
Una vez creado, abrir el archivo `.env` reci√©n creado y agregar la siguiente configuraci√≥n:
```shell
GOOGLE_APPLICATION_CREDENTIALS=/app/env-api/gen-lang-client.json
```
### Obtener credenciales de servicio
1. **Acceder a Google Cloud Console**
   - Ir a [Google Cloud Console](https://console.cloud.google.com/)
   - Navegar a **IAM & Admin** ‚Üí **Service Accounts**

2. **Crear/seleccionar cuenta de servicio**
   - Crear una nueva cuenta de servicio o seleccionar una existente
   - Asegurar que tenga permisos para usar Gemini API

3. **Generar clave JSON**
   - Ir a la pesta√±a **"Keys"**
   - Clic en **"Add key"** ‚Üí **"Create new key"**
   - Seleccionar formato **JSON**
   - Descargar el archivo (solo disponible una vez)

4. **Configurar archivo de credenciales**
   - Renombrar el archivo descargado a `gen-lang-client.json`
   - Mover el archivo a `copiloto-conversacional/env-api/`

#### Estructura final esperada
```
copiloto-conversacional/
‚îú‚îÄ‚îÄ env-api/
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îî‚îÄ‚îÄ gen-lang-client.json
....
```
---

## üê≥ Levantar el proyecto con Docker
Para ejecutar el proyecto con docker abra la CMD en la ruta ra√≠z del proyecto `copiloto-conversacional\` e ingrese el comando:
```shell
docker-compose up --build
```
---

## üìê Arquitectura del Sistema
### Diagrama de arquitectura de la soluci√≥n
<img width="420" height="420" alt="Diagramas tesis-Arquitectura IA Copiloto drawio" src="https://github.com/user-attachments/assets/07487585-0087-4798-a976-e41b94a3147b" />

---

## üí°Justificaciones t√©cnicas

### Backend: Flask
Se eligi√≥ Flask por su simplicidad y flexibilidad al momento de crear APIs ligeras. Es f√°cil de integrar con librer√≠as externas (como LangChain o ChromaDB) y permite un r√°pido desarrollo. Adem√°s de poseer experiencia previa con esta tecnolog√≠a.
### Frontend: React
React fue seleccionado por su modularidad y la facilidad para construir interfaces din√°micas e interactivas. Adem√°s de poseer experiencia previa con esta tecnolog√≠a.

### Framework: LangChain
LangChain se emplea como capa de orquestaci√≥n para conectar modelos de lenguaje con el almacenamiento vectorial (ChromaDB). Su capacidad para trabajar con cadenas de prompts y manejar grandes vol√∫menes de texto lo hace ideal para este sistema.

### Vector DB: ChromaDB
Se eligi√≥ ChromaDB como base de datos vectorial por su integraci√≥n sencilla con LangChain, su eficiencia en b√∫squedas sem√°nticas y su dise√±o pensado espec√≠ficamente para aplicaciones de IA.

### Modelo generativo: Google Gemini (Gemini 2.0)
Gemini 2.0 se seleccion√≥ como modelo principal por su capacidad de comprensi√≥n en lenguaje natural y su habilidad para generar respuestas contextuales y precisas. Adem√°s, de ser un modelo gratuito y facil de implementar en LangChain.

---

## üåä Flujo y funcionamiento del sistema
### Subida de documentos
El usuario puede subir hasta 5 PDFs al sistema. Una vez que un archivo llega al backend en Flask, este lo procesa para extraer su contenido en texto. 
Si el PDF no tiene texto legible (por ejemplo, si es solo una imagen), se notifica como error

### Procesamiento y divisi√≥n en fragmentos
El texto extra√≠do no se guarda como un bloque gigante, sino que se divide en fragmentos m√°s peque√±os (chunks). Esto se hace para que despu√©s sea m√°s f√°cil buscar informaci√≥n espec√≠fica dentro del documento y para que el modelo de IA pueda trabajar con partes manejables del texto

###  Clasificaci√≥n por temas
Cada texto se analiza con ayuda del modelo Gemini, que intenta detectar el tema principal (ejemplo: "recursos humanos", "educaci√≥n", "tecnolog√≠a"). Esa etiqueta de tema se guarda junto con el fragmento, como metadato

###  Almacenamiento en la base de conocimiento (ChromaDB)
Todos los fragmentos de todos los documentos se guardan en una base vectorial llamada ChromaDB. Esta base permite que, en lugar de hacer una b√∫squeda por palabras exactas, se pueda hacer una b√∫squeda "sem√°ntica", es decir, encontrar texto con un significado similar a lo que pregunt√≥ el usuario

### Consultas del usuario
Cuando el usuario hace una pregunta en el chat (ejemplo: "¬øCu√°l es la experiencia laboral de Mois√©s?"), el sistema primero busca en ChromaDB los fragmentos de texto m√°s relevantes. Luego, esos fragmentos se env√≠an junto con la consulta al modelo Gemini. Gemini responde tomando en cuenta la pregunta y la informaci√≥n de los documentos que se hab√≠an subido

### Comparaci√≥n de documentos
El usuario tambi√©n puede seleccionar varios documentos (por ejemplo, dos CVs) y pedir una comparaci√≥n autom√°tica. En este caso, el backend extrae fragmentos relevantes de cada documento, se los pasa al modelo Gemini y este genera un an√°lisis de las similitudes y diferencias

### B√∫squeda por temas
Como cada texto est√° clasificado por un tema principal, el usuario puede pedir: "Mu√©strame los documentos relacionados con educaci√≥n". El sistema filtra en ChromaDB usando esas etiquetas de temas y devuelve los documentos que coinciden

### Interfaz de usuario
En el frontend, el chat muestra los mensajes en orden: primero lo que escribe el usuario y luego la respuesta del modelo. Cuando se suben archivos, aparecen listados, y el usuario puede marcar con checkboxes cu√°les quiere usar para ciertas consultas (como comparaciones)

---

## üîß Limitaciones y mejoras futuras 

### Limitaciones de la soluci√≥n

- Actualmente, la soluci√≥n est√° pensada para ejecutarse en PC y no est√° optimizada para dispositivos m√≥viles.
- La b√∫squeda sem√°ntica con ChromaDB puede verse limitada seg√∫n el formato del PDF (por ejemplo, si el documento contiene muchas tablas, im√°genes o texto escaneado sin OCR).
- La asignaci√≥n de temas a los documentos se basa √∫nicamente en los dos primeros fragmentos (chunks), que luego se replican para el resto del texto. Esto reduce el consumo del modelo Gemini, pero afecta la precisi√≥n de la clasificaci√≥n tem√°tica.
- La comparaci√≥n entre documentos tambi√©n se hace tomando solo los dos primeros fragmentos de cada uno. Esto permite ahorrar recursos del modelo Gemini, pero no garantiza un an√°lisis completo ni detallado.

### Posibles mejoras futuras

- Despliegue en la web: Publicar la aplicaci√≥n en un servidor o nube (ejemplo: AWS, Azure, GCP, Vercel) para que se pueda usar desde cualquier dispositivo.
- Compatibilidad m√≥vil: Adaptar la interfaz para celulares y tablets.
- OCR avanzado: Incorporar reconocimiento √≥ptico de caracteres para extraer texto de PDFs escaneados o con im√°genes.
- Clasificaci√≥n m√°s precisa: Asignar temas de manera din√°mica a cada fragmento (no solo a los primeros) para mejorar la coherencia y granularidad de la informaci√≥n.
- Comparaci√≥n m√°s completa: Analizar m√∫ltiples fragmentos de cada documento en lugar de solo los primeros, logrando comparaciones m√°s ricas y contextuales.
- Optimizaci√≥n del consumo de IA: Implementar cacheo de resultados y embeddings pre-generados para reducir llamadas innecesarias a Gemini.
- Interfaz interactiva: Agregar filtros, res√∫menes autom√°ticos y visualizaci√≥n de documentos por temas.
- Soporte multilenguaje: Permitir b√∫squedas y respuestas en distintos idiomas.
- Integraci√≥n con otras fuentes: Conectar el sistema no solo a PDFs, sino tambi√©n a Word, Excel o incluso bases de datos empresariales.

---

## üìÑ Licencia

Este proyecto est√° bajo la Licencia GPL-3.0 license. Ver archivo `LICENSE.txt` para m√°s detalles.

---

## üìß Contacto
- **Desarrollador**: Mois√©s Godoy
- **Perfil**: [LinkedIn](https://www.linkedin.com/in/moises-andres-godoy-carre√±o-58b4a4370)
- **Repositorio**: [GitHub](https://github.com/MoisesGodoy17/)
- **Correo**: moisesgodoy1704@outlook.com




